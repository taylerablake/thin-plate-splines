\documentclass[12pt]{article}
\usepackage{graphicx,psfrag,float,mathbbol,xcolor,cleveref}
\usepackage{arydshln}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage[mathscr]{euscript}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{natbib} 
\usepackage[T1]{fontenc}
\usepackage{enumitem}
\usepackage{accents}
\usepackage{framed}
\usepackage{subcaption}
\usepackage{mathtools}
\usepackage{IEEEtrantools}
\usepackage{times}
\usepackage{amsthm}
\usepackage{tabularx,ragged2e,booktabs,caption}


\newcolumntype{C}[1]{>{\Centering}m{#1}}
\renewcommand\tabularxcolumn[1]{C{#1}}
\usepackage[letterpaper, left=1in, top=1in, right=1in, bottom=1in,nohead,includefoot, verbose, ignoremp]{geometry}
\newcommand{\comment}[1]{\text{\phantom{(#1)}} \tag{#1}}
\newcommand{\ms}{\scriptscriptstyle}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand*\needsparaphrased{\color{red}}
\newcommand*\needscited{\color{orange}}
\newcommand*\needsproof{\color{blue}}
\newcommand*\outlineskeleton{\color{green}}
\newcommand{\PP}{\mathcal{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bfeps}{\mbox{\boldmath $\epsilon$}}
\newcommand{\bfgamma}{\mbox{\boldmath $\gamma$}}
\newcommand{\bflam}{\mbox{\boldmath $\lambda$}}
\newcommand{\bfphi}{\mbox{\boldmath $\phi$}}
\newcommand{\bfsigma}{\mbox{\boldmath $\sigma$}}
\newcommand{\bfbeta}{\mbox{\boldmath $\beta$}}
\newcommand{\bfalpha}{\mbox{\boldmath $\alpha$}}
\newcommand{\bfe}{\mbox{\boldmath $e$}}
\newcommand{\bff}{\mbox{\boldmath $f$}}
\newcommand{\bfone}{\mbox{\boldmath $1$}}
\newcommand{\bft}{\mbox{\boldmath $t$}}
\newcommand{\bfo}{\mbox{\boldmath $0$}}
\newcommand{\bfO}{\mbox{\boldmath $O$}}
\newcommand{\bfx}{\mbox{\boldmath $x$}}
\newcommand{\bfX}{\mbox{\boldmath $X$}}
\newcommand{\bfz}{\mbox{\boldmath $z$}}


\newcommand{\bfm}{\mbox{\boldmath $m}}
\newcommand{\bfy}{\mbox{\boldmath $y$}}
\newcommand{\bfa}{\mbox{\boldmath $a$}}
\newcommand{\bfb}{\mbox{\boldmath $b$}}
\newcommand{\bfY}{\mbox{\boldmath $Y$}}
\newcommand{\bfS}{\mbox{\boldmath $S$}}
\newcommand{\bfZ}{\mbox{\boldmath $Z$}}
\newcommand{\cardT}{\vert \mathcal{T} \vert}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]
\def\bL{\mathbf{L}}

\bibliographystyle{abbrvnat}

\begin{document}

\title{Bivariate Thin-plate Splines Models for Nonparametric Covariance Estimation with Longitudinal Data}

\author{Tayler A. Blake\thanks{The Ohio State University, 1958 Neil Avenue, Columbus, OH 43201} \and  Yoonkyung Lee\thanks{The Ohio State University, 1958 Neil Avenue, Columbus, OH 43201}}

\maketitle

The theoretical foundations of the thin-plate spline was laid in the seminal work of \citet{duchon1977splines}. For a bivariate function $f\left(x_1,x_1\right)$, the usual thin-plate spline functional ($d=m=2$) is given by

\begin{equation} \label{eq:thin-plate-order-2-penalty}
J_2 \left(f \right) = \int \limits_{-\infty}^{\infty} \int \limits_{-\infty}^{\infty}  \left( f_{{x_1} {x_1}}^2 + f_{{x_1} {x_2}}^2 + f_{{x_2} {x_2}}^2  \right) dx_1 dx_2
\end{equation}

and in general, 
\begin{equation} \label{eq:thin-plate-order-m-penalty}
J_m \left(f \right) = \sum_{\nu=0}^m \int \limits_{-\infty}^{\infty} \int \limits_{-\infty}^{\infty}  {m \choose \nu} \left(  \frac{\partial^m f}{\partial x_1^\nu \partial x_2^{m-\nu}}  \right)^2 dx_1 dx_2 .
\end{equation}

For $d=2$, define the inner product of functions $f$ and $g$ as follows:

\begin{equation} \label{eq:thin-plate-inner-product}
\langle f,g \rangle = \sum_{\alpha_1 + \alpha_2=m} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \left(  \frac{\partial^m f}{\partial x_1^{\alpha_1} \partial x_2^{\alpha_2}}  \right)\left(  \frac{\partial^m g}{\partial x_1^{\alpha_1} \partial x_2^{\alpha_2}}  \right) dx_1 dx_2 .
\end{equation}

\bigskip

We suppose that $f \in \mathcal{X}$, the space of functions with partial derivatives of total order $m$ belong to $\mathcal{L}_2\left(E^2\right)$. We endow $\mathcal{X}$ with seminorm $J^2_m\left( f \right)$; for such $\mathcal{X}$ to be a reproducing kernel Hilbert space, i.e. for the evaluation functionals to be bounded in $\mathcal{X}$, if it necessary and sufficient that $2m > d$. For $d=2$, we require $m>1$.

\bigskip
The data model for a random vector $y_i = \left(y_{i1},\dots, y_{i,M_i} \right)^\prime$ is given by 

\begin{equation} \label{eq:functional-vc-ar-model}
y_{ij} = \sum_{k<j} \phi^*\left(v_{ijk}\right) y_{ik} + \sigma\left(v_{ijk}\right) e_{ij}
\end{equation}
\noindent
where $v_{ijk} = \left(t_{ij}-t_{ik}, \frac{1}{2}\left(t_{ij}+t_{ik}\right)\right) = \left(l_{ijk}, m_{ijk}\right)$. We assume that $\phi^* \in \mathcal{X}$ and $e_{ij} \stackrel{\text{i.i.d.}}{\sim} N\left(0,1\right)$. If we have a random sample of observed vectors $y_1,\dots,y_N$ available for estimating $\phi^*$, then we take $\phi^*$ to be the minimizer of
\begin{equation} \label{eq:thin-plate-loss-function}
Q_\lambda \left( \phi^* \right) = \sum_{i=1}^N \sigma^{-2}_{ijk}\left( y_{ij} - \sum_{k<j} \phi^*\left(v_{ijk}\right) y_{ik}  \right)^2 + \lambda J_m^2 \left( \phi^* \right)
\end{equation}
\noindent
where $\sigma^{2}_{ijk} = \sigma^2\left(v_{ijk}\right)$. The null space of the penalty functional $J_m^2 \left( \phi^* \right)$, denoted $\mathcal{H}_0$, corresponds to the $d_0={2+m-1 \choose 2}$-dimensional space spanned by the polynomials in two variables of total degree $< m$. For example, for $d=m=2$, we have that $d_0=3$, and the null space of $J^2_2$ is spanned by $\eta_1$, $\eta_2$, and $\eta_3$ where

\[
\eta_1\left(v\right) = 1,\quad \eta_2\left(v\right) = l, \quad \eta_2\left(v\right) = m.
\]
\noindent
In general, we let $\eta_1,\dots, \eta_{d_0}$ denote the $d_0$ monomials of total degree less than $m$.

\bigskip

\citet{duchon1977splines} showed that if the $\left\{ v_{ijk} \right\}$ are such that the least squares regression of $\left\{ y_{ijk}\right\}$ on $\eta_1,\dots, \eta_{d_0}$ is unique, then there exists a unique minimizer of \ref{eq:thin-plate-loss-function}, $\phi^*_\lambda$, which has the form

\begin{equation} \label{eq:unique-minimizer-of-loss}
\phi^*_\lambda\left( v \right) = \sum_{\nu=0}^{d_0} d_\nu \eta_\nu \left( v \right) + \sum_{v_i \in \mathcal{V}} c_{i} E_m\left(v,v_i \right)
\end{equation}
\noindent
where $\mathcal{V}$ denotes the set of unique within-subject pairs of observed $\left\{ v_{ijk} \right\}$. $E_m$ is a Green's function of the $m$-iterated Laplacian. Let

\begin{align} \label{eq:thin-plate-pseudo-kernel-1}
E_m\left( \tau \right) = \left \{ \begin{array}{ll}   \theta_{m,d} \vert \tau \vert^{2m-d}\log \vert \tau \vert  &  \quad 2m-d \mbox{\; even} \\
					   \theta_{m,d} \vert \tau \vert^{2m-d}& \quad 2m-d \mbox{\; odd} \end{array} \right.
\end{align}

\begin{align} \label{eq:thin-plate-pseudo-kernel-2}
\theta_{md} = \left \{ \begin{array}{cl}   \frac{ \left( -1 \right)^{\frac{d}{2} + 1 + m} }{ 2^{2m-1}\pi^{\frac{d}{2}}\left(m-1\right)! \left( m - \frac{d}{2} \right)! }   & \quad 2m-d \mbox{\; even} \\
					   \frac{ \Gamma\left( \frac{d}{2} - m\right) }{ 2^{2m}\pi^{\frac{d}{2}}\left(m-1\right)! }    & \quad 2m-d \mbox{\; odd} \end{array} \right.
\end{align}

Defining $\vert v - v_i \vert = \bigg[ \left( l - l_i \right)^2 + \left( m - m_i \right)^2  \bigg]^{1/2}$, then we can write 
\[
E_m\left( v,\tilde{v} \right) = E_m\left(\vert v-\tilde{v} \vert \right) 
\]
\noindent
Formally, we have that 
\[
\Delta^m E_m\left( \cdot,v_i \right) = \delta_{v_i}, 
\]
so 
\[
\Delta^m \phi^*_\lambda \left( v \right) = 0 \;\; \mbox{for}\;\;v \ne v_i, \;\;i=1,\dots, n 
\]
\noindent
where $n = \vert \mathcal{V} \vert$.

\bigskip
The kernel $E_m$ is not positive definite, but rather \emph{conditionally positive definite}....


Stack the $N$ observed response vectors $y_1,\dots, y_N$ less their first element $y_{i1}$ into a single vector $Y$ of dimension $n_y=\left(\sum \limits_{i} M_i \right) - N$.  Let $B$ denote the $n \times d_0$ matrix with $i$-$\nu^{th}$ element $\eta_\nu\left(v_i\right)$, which we assume has full column rank; let $K$ denote the $n \times n$ kernel matrix with $i$-$j^{th}$ element $E_m\left(v_i, v_j\right)$, and let $D$ denote the $n_y \times n_y$ diagonal matrix of innovation variances $\sigma^2_{ijk}$. The $\phi^*$ minimizing \ref{eq:thin-plate-loss-function} corresponds to the coefficient vectors $c$, $d$ minimizing

\begin{equation} \label{eq:thin-plate-loss-function-matrix} 
\left( Y - W \left( Bd + Kc \right) \right)^\prime D^{-1} \left( Y - W \left( Bd + Kc \right) \right) + c^\prime K c 
\end{equation} 
\noindent
subject to $B^\prime c = 0$, where $W$ is the matrix of autoregressive covariates constructed so that $\ref{eq:thin-plate-loss-function}$ and $\ref{eq:thin-plate-loss-function-matrix}$ are equivalent. 
\bigskip
Differentiating $Q_\lambda$ with respect to $c$ and $d$ and setting equal to zero, we have that 

\begin{align}
\frac{\partial Q_\lambda}{\partial c} = K W^\prime D^{-1}\left[ W\left(Bd + Kc\right) - Y  \right] + \lambda Kc &= 0 \nonumber \\
%\Longleftrightarrow    W^\prime D^{-1} W \left( Bd + Kc\right) + \lambda c &= W^\prime D^{-1} Y \\
\iff    W'D^{-1} W \bigg[ Bd + Kc \bigg] + \lambda c  &= W' D^{-1}Y \label{eq:normal-eq-1}
\end{align}
\noindent

\begin{align*}
\frac{\partial Q_\lambda}{\partial d} = B^\prime W^\prime D^{-1}\left[ W\left(Bd + Kc\right) - Y  \right] &=0 \nonumber \\
%\Longleftrightarrow    W^\prime D^{-1} W \left( Bd + Kc\right) + \lambda c &= W^\prime D^{-1} Y \\
\iff   - \lambda B' c  &= 0  
\end{align*}
\noindent

So, the coefficients satisfy the normal equations
\begin{align} 
Y &= W \bigg[ Bd + \left(K  + \lambda \left(W^\prime D^{-1} W \right)^{-1} \right) c \bigg] \label{eq:simple-normal-eq-1} \\
B' c  &= 0  \label{eq:simple-normal-eq-2}
\end{align}
\noindent
Using the QR decomposition of $B$, we may write 

\begin{equation*}
B = QR = \begin{bmatrix} Q_1 &  Q_2 \end{bmatrix} \begin{bmatrix} R \\  0 \end{bmatrix} = Q_1 R
\end{equation*}
\noindent
where $Q$ is an orthogonal matrix; $Q_1$ has dimension $n \times d_0$,  and $Q_2$ has dimension $n \times \left(n-d_0\right)$. Since $B'c = 0$, $c$ must belong to the subspace spanned by the columns of $Q_2$, so 

\[
c = Q_2 \gamma
\] 
\noindent
for some $\gamma \in \mathrm{R}^{n-d_0}$. The matrix $M = W' D^{-1} W$ is full rank as long as $n_y \ge n$, so \ref{eq:normal-eq-1} becomes
\begin{equation} \label{eq:solve-for-gamma}
\left(W'D^{-1}W\right)^{-1} W' D^{-1}Y =  Bd + \bigg[K + \lambda \left(W'D^{-1}W\right)^{-1}  \bigg]  Q_2 \gamma. 
\end{equation}
\noindent
Premultiplying both sides of \ref{eq:solve-for-gamma} by $Q_2'$, we have that
\begin{equation} \label{eq:solve-for-c}
c = Q_2 \bigg[ Q_2' \left( K + \lambda \left( W'D^{-1} W \right)^{-1} \right)Q_2 \bigg]^{-1} Q_2' \left( W'D^{-1} W \right)^{-1}W'D^{-1} Y
\end{equation}
\noindent
Using $B = Q_1 R$, we can write
\begin{equation} \label{eq:solve-for-d}
d = R^{-1} Q_1' \bigg[ \left( W'D^{-1} W \right)^{-1} Y - \left( K + \lambda \left( W'D^{-1} W \right)^{-1} \right) c  \bigg]
\end{equation}



\bibliography{Master}

\end{document}
